# Arquiteturas para Web Scraping (Playwright + Cheerio)

Um guia comparativo de arquiteturas para construir scrapers robustos, escalÃ¡veis e mantÃ­veis.

## ğŸŒ EstratÃ©gias de RenderizaÃ§Ã£o (CSR vs SSR)

Antes de escolher arquitetura, escolha o **motor de extraÃ§Ã£o por tipo de site**:

| Tipo de site | Sinal comum | EstratÃ©gia recomendada | Stack sugerida |
|---|---|---|---|
| **SSR/estÃ¡tico** | HTML jÃ¡ contÃ©m os dados no `view-source` | RequisiÃ§Ã£o HTTP + parse de HTML | `fetch` + `cheerio` |
| **CSR/SPA** | HTML inicial vazio e dados apÃ³s JS | AutomaÃ§Ã£o de browser | `playwright` |
| **HÃ­brido** | Parte no HTML, parte via API/JS | Tentar SSR primeiro, fallback browser | `cheerio` + `playwright` |

### Regra prÃ¡tica

- Comece com `cheerio` para pÃ¡ginas SSR (mais rÃ¡pido e barato).
- Use `playwright` apenas quando o conteÃºdo depender de JavaScript, login complexo, anti-bot visual ou interaÃ§Ã£o de UI.
- Em pipelines longos, priorize o modelo hÃ­brido: **HTTP-first, browser-fallback**.

## ğŸ“Š AnÃ¡lise do Projeto "QueroDADOS"


### Problemas Identificados

O projeto `querodados` apresenta boas caracterÃ­sticas, mas com alguns desafios:

**âœ… Pontos Fortes:**
- Scraper especÃ­fico por portal (OLX, ZAP)
- GitHub Actions para automaÃ§Ã£o
- DocumentaÃ§Ã£o bÃ¡sica
- ConfiguraÃ§Ãµes por portal

**âŒ Pontos Fracos:**
- Entry point Ãºnico (`index.js`) com lÃ³gica if/else
- Scrapers nÃ£o compartilham estrutura comum
- Sem validaÃ§Ã£o de dados centralizada
- Sem sistema de retry/fallback
- Sem tipagem TypeScript
- Sem tratamento robusto de erros
- Sem logging estruturado
- Dados salvos mas sem pipeline clara
- Sem testes unitÃ¡rios/E2E
- Estrutura cresce desordenadamente com novos scrapers

---

## ğŸ—ï¸ Arquitetura 1: Modular Simples (Recomendado para comeÃ§ar)

Ideal para 1-3 scrapers simples com requisitos bÃ¡sicos.

```
scrapers/
â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ BaseScraper.ts          # Classe abstrata
â”‚   â””â”€â”€ types.ts                 # Tipos compartilhados
â”œâ”€â”€ olx/
â”‚   â”œâ”€â”€ OlxScraper.ts
â”‚   â”œâ”€â”€ selectors.ts
â”‚   â””â”€â”€ mapper.ts
â”œâ”€â”€ zap/
â”‚   â”œâ”€â”€ ZapScraper.ts
â”‚   â”œâ”€â”€ selectors.ts
â”‚   â””â”€â”€ mapper.ts
â””â”€â”€ index.ts                     # Factory/Router

utils/
â”œâ”€â”€ logger.ts                    # Logging estruturado
â”œâ”€â”€ validator.ts                 # ValidaÃ§Ã£o de dados
â”œâ”€â”€ retry.ts                     # Retry logic
â””â”€â”€ storage.ts                   # PersistÃªncia

config/
â”œâ”€â”€ index.ts                     # Config centralizada
â”œâ”€â”€ scrapers.ts                  # Config por scraper
â””â”€â”€ env.ts                       # VariÃ¡veis de ambiente

main.ts                          # Entry point limpo
```

**Pros:**
- âœ… Simples de entender e comeÃ§ar
- âœ… FÃ¡cil adicionar novo scraper
- âœ… CÃ³digo reutilizÃ¡vel com BaseScraper
- âœ… ConfiguraÃ§Ã£o centralizada

**Contras:**
- âŒ Cresce mal com muitos scrapers (10+)
- âŒ Sem separaÃ§Ã£o por domÃ­nio
- âŒ DifÃ­cil escalar para microserviÃ§os

**Quando usar:**
- Projeto novo com 1-3 scrapers
- Prototipagem rÃ¡pida
- Time pequeno

**OpÃ§Ã£o SSR (Cheerio):**
- Crie `BaseHttpScraper` paralela Ã  `BaseScraper` de Playwright.
- Para portais SSR, implemente scraper com `fetch` + `cheerio` para reduzir custo e tempo.
- Mantenha o mesmo contrato de saÃ­da para o pipeline de merge/validaÃ§Ã£o.

---

## ğŸ—ï¸ Arquitetura 2: Domain-Driven Design (DDD)

Ideal para 5+ scrapers ou domÃ­nios diferentes.

```
src/
â”œâ”€â”€ domains/                     # Contextos por domÃ­nio
â”‚   â”œâ”€â”€ imobiliario/
â”‚   â”‚   â”œâ”€â”€ application/
â”‚   â”‚   â”‚   â”œâ”€â”€ ScrapeImovelUseCase.ts
â”‚   â”‚   â”‚   â””â”€â”€ ProcessImovelUseCase.ts
â”‚   â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”‚   â”œâ”€â”€ Imovel.ts       # Entity
â”‚   â”‚   â”‚   â”œâ”€â”€ ImovelRepository.ts
â”‚   â”‚   â”‚   â””â”€â”€ events/
â”‚   â”‚   â”œâ”€â”€ infrastructure/
â”‚   â”‚   â”‚   â”œâ”€â”€ OlxScraper.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ ZapScraper.ts
â”‚   â”‚   â”‚   â””â”€â”€ FileStorage.ts
â”‚   â”‚   â””â”€â”€ http/               # Controllers
â”‚   â”œâ”€â”€ veiculo/
â”‚   â”‚   â”œâ”€â”€ application/
â”‚   â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â””â”€â”€ infrastructure/
â”‚   â””â”€â”€ ...outros domÃ­nios
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”œâ”€â”€ Result.ts
â”‚   â”‚   â””â”€â”€ DomainEvent.ts
â”‚   â”œâ”€â”€ infrastructure/
â”‚   â”‚   â”œâ”€â”€ Logger.ts
â”‚   â”‚   â””â”€â”€ Config.ts
â”‚   â””â”€â”€ http/
â”‚       â””â”€â”€ StatusController.ts
â”œâ”€â”€ main.ts
â””â”€â”€ container.ts                # Dependency Injection
```

**Pros:**
- âœ… Muito escalÃ¡vel
- âœ… FÃ¡cil adicionar domÃ­nios (imÃ³veis, veÃ­culos, etc)
- âœ… CÃ³digo independente por domÃ­nio
- âœ… Pronto para microserviÃ§os
- âœ… Testabilidade excelente

**Contras:**
- âŒ Complexidade inicial alta
- âŒ Curva de aprendizado
- âŒ Pode ser overkill para projeto pequeno
- âŒ Mais boilerplate

**Quando usar:**
- Projeto vai ter mÃºltiplos domÃ­nios
- Time experiente com DDD
- Scaling Ã© prioridade
- Longo prazo

**OpÃ§Ã£o SSR (Cheerio):**
- Trate `cheerio` como adapter de infraestrutura HTTP.
- Deixe a decisÃ£o Playwright/Cheerio fora do domÃ­nio, dentro da camada de adapters.
- Permita fallback para Playwright quando a extraÃ§Ã£o SSR falhar.

---

## ğŸ—ï¸ Arquitetura 3: Plugin-Based

Ideal para 10+ scrapers ou sistema extensÃ­vel.

```
src/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ ScraperPlugin.interface.ts
â”‚   â”œâ”€â”€ PipelineOrchestrator.ts
â”‚   â”œâ”€â”€ EventBus.ts
â”‚   â””â”€â”€ Registry.ts
â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ olx/
â”‚   â”‚   â”œâ”€â”€ OlxPlugin.ts
â”‚   â”‚   â”œâ”€â”€ OlxScraper.ts
â”‚   â”‚   â”œâ”€â”€ OlxMapper.ts
â”‚   â”‚   â””â”€â”€ olx.config.ts
â”‚   â”œâ”€â”€ zap/
â”‚   â”‚   â”œâ”€â”€ ZapPlugin.ts
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ immobiliare/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ registry.ts
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ steps/
â”‚   â”‚   â”œâ”€â”€ ValidateStep.ts
â”‚   â”‚   â”œâ”€â”€ EnrichStep.ts
â”‚   â”‚   â”œâ”€â”€ DeduplicateStep.ts
â”‚   â”‚   â””â”€â”€ StorageStep.ts
â”‚   â””â”€â”€ executor.ts
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ types/
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ storage/
â””â”€â”€ main.ts
```

**Pros:**
- âœ… Altamente extensÃ­vel
- âœ… Plugins independentes
- âœ… FÃ¡cil remover/adicionar scrapers
- âœ… Hot-reload possÃ­vel
- âœ… Pronto para distribuiÃ§Ã£o

**Contras:**
- âŒ Complexidade muito alta
- âŒ Mais difÃ­cil de debugar
- âŒ Requer padrÃµes rÃ­gidos

**Quando usar:**
- Plataforma scraper (app terceiros)
- Muitos scrapers heterogÃªneos
- SaaS scraping platform

**OpÃ§Ã£o SSR (Cheerio):**
- Separe plugins por engine: `*.http.plugin` (Cheerio) e `*.browser.plugin` (Playwright).
- Registre metadados de capacidade (SSR, JS-heavy, auth) para roteamento automÃ¡tico.
- Priorize execuÃ§Ã£o dos plugins HTTP para ganhar throughput.

---

## ğŸ—ï¸ Arquitetura 4: Queue-Based (Async Job Processing)

Ideal para scraping em larga escala com agendamento.

```
src/
â”œâ”€â”€ queues/
â”‚   â”œâ”€â”€ ScraperQueue.ts
â”‚   â”œâ”€â”€ ProcessorQueue.ts
â”‚   â””â”€â”€ workers/
â”‚       â”œâ”€â”€ scraperWorker.ts
â”‚       â”œâ”€â”€ validatorWorker.ts
â”‚       â””â”€â”€ storageWorker.ts
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ BaseScraper.ts
â”‚   â”œâ”€â”€ olx/
â”‚   â””â”€â”€ zap/
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ ScrapeJob.ts
â”‚   â”œâ”€â”€ ProcessJob.ts
â”‚   â””â”€â”€ DeliveryJob.ts
â”œâ”€â”€ scheduler/
â”‚   â”œâ”€â”€ Scheduler.ts
â”‚   â””â”€â”€ cron-jobs/
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ MetricsCollector.ts
â”‚   â””â”€â”€ HealthCheck.ts
â””â”€â”€ main.ts
```

**Pros:**
- âœ… Escalabilidade horizontal
- âœ… Retry automÃ¡tico
- âœ… Processamento assÃ­ncrono
- âœ… Agendamento robusto
- âœ… Monitoramento built-in

**Contras:**
- âŒ Infra complexa (Redis, RabbitMQ, etc)
- âŒ Debugging difÃ­cil
- âŒ Requer DevOps
- âŒ Overkill para pequenos projetos

**Quando usar:**
- Centenas de scrapers
- Scraping 24/7 em produÃ§Ã£o
- MÃºltiplos workers/mÃ¡quinas
- SLA importante

**OpÃ§Ã£o SSR (Cheerio):**
- Crie filas distintas: `http-queue` (Cheerio) e `browser-queue` (Playwright).
- Aloque mais workers para HTTP e menos para browser (custos menores e maior volume).
- Use retry agressivo em HTTP e retry mais conservador em browser.

---

## ğŸ“‹ ComparaÃ§Ã£o RÃ¡pida

| Aspecto | Modular | DDD | Plugin | Queue |
|--------|---------|-----|--------|-------|
| **Simplicidade** | â­â­â­â­â­ | â­â­ | â­â­ | â­ |
| **Escalabilidade** | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| **Testabilidade** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| **Performance (6h)** | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­ |
| **Fit SSR (Cheerio)** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| **Curva Aprendizado** | â­â­â­â­â­ | â­â­â­ | â­â­ | â­â­ |
| **Infra NecessÃ¡ria** | Minimal | Minimal | Minimal | Complexa |
| **Melhor Para** | ComeÃ§o | MÃ©dio prazo | ExtensÃ­vel | ProduÃ§Ã£o |

---

## âš¡ Performance em Pipeline de 6 Horas

### Contexto: GitHub Actions/GitLab CI com Limite de Tempo

Se seus scrapers rodam em **pipelines de no mÃ¡ximo 6 horas**, as prioridades mudam drasticamente:

#### âŒ **Queue-Based NÃƒO Ã© recomendado**
- Overhead de Redis/RabbitMQ desperdiÃ§a tempo
- NÃ£o hÃ¡ necessidade de processamento assÃ­ncrono
- Complexidade desnecessÃ¡ria para job Ãºnico

#### âœ… **RecomendaÃ§Ã£o: Modular + ParalelizaÃ§Ã£o + HTTP-first (Cheerio)**

```typescript
// main.ts - ExecuÃ§Ã£o paralela
const scrapers = [
  new OlxScraper(),
  new ZapScraper(),
  new VivaRealScraper(),
];

// Executar todos em paralelo
await Promise.allSettled(
  scrapers.map(scraper => scraper.run())
);
```

**Estrutura Otimizada para Pipeline:**

```
src/
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ BaseScraper.ts          # Playwright compartilhado
â”‚   â”‚   â”œâ”€â”€ BaseHttpScraper.ts      # SSR com fetch + cheerio
â”‚   â”‚   â””â”€â”€ ParallelExecutor.ts     # Orquestrador paralelo
â”‚   â”œâ”€â”€ olx/
â”‚   â”œâ”€â”€ zap/
â”‚   â””â”€â”€ index.ts
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ orchestrator.ts              # Controla execuÃ§Ã£o
â”‚   â”œâ”€â”€ timeout-handler.ts           # Gerencia limite de 6h
â”‚   â””â”€â”€ checkpoint.ts                # Salva progresso
â””â”€â”€ main.ts

scripts/
â”œâ”€â”€ run-all.sh                       # Script CI/CD
â””â”€â”€ monitor.sh                       # Tracking de tempo
```

### ğŸš€ EstratÃ©gias para Maximizar Performance em 6h

#### 1. **ParalelizaÃ§Ã£o Inteligente (HTTP primeiro)**

```typescript
// âŒ Ruim - Sequencial (desperdiÃ§a tempo)
for (const scraper of scrapers) {
  await scraper.run(); // 1h cada = 5h total
}

// âœ… Bom - Paralelo com controle
const results = await Promise.allSettled(
  scrapers.sort((a, b) => Number(a.requiresBrowser) - Number(b.requiresBrowser))
    .map(async (scraper) => {
    const timeout = setTimeout(() => {
      scraper.cancel(); // Cancela apÃ³s 5h
    }, 5 * 60 * 60 * 1000);
    
    try {
      return await scraper.run();
    } finally {
      clearTimeout(timeout);
    }
    })
);
```

#### 2. **PriorizaÃ§Ã£o com Time Budget**

```typescript
// Scraper com prioridade e budget
interface ScraperConfig {
  priority: number;      // 1 = alta, 5 = baixa
  maxDuration: number;   // ms
  essential: boolean;    // Se falhar, todo pipeline falha
}

const scrapers = [
  { scraper: olx, priority: 1, maxDuration: 2h, essential: true },
  { scraper: zap, priority: 1, maxDuration: 2h, essential: true },
  { scraper: imovelweb, priority: 2, maxDuration: 1h, essential: false },
];

// Executar por prioridade com timeout
await executeWithPriority(scrapers, totalBudget: 6h);
```

#### 3. **Checkpointing para Re-runs**

```typescript
// Se pipeline quebrar, continua de onde parou
class CheckpointManager {
  async saveProgress(scraper: string, data: any) {
    await fs.writeFile(
      `checkpoints/${scraper}.json`,
      JSON.stringify({ lastPage, processedIds, timestamp })
    );
  }
  
  async loadProgress(scraper: string) {
    // Continua do checkpoint
  }
}
```

#### 4. **Browser Reuse (Economia de Tempo)**

```typescript
// âŒ Ruim - Abre browser a cada scraper (lento)
class OlxScraper {
  async run() {
    const browser = await playwright.chromium.launch();
    // ...
    await browser.close();
  }
}

// âœ… Bom - Reutiliza contexto
class ScraperPool {
  private browser: Browser;
  
  async initialize() {
    this.browser = await playwright.chromium.launch();
  }
  
  async runScraper(scraper: BaseScraper) {
    const context = await this.browser.newContext();
    await scraper.run(context);
    await context.close();
  }
  
  async cleanup() {
    await this.browser.close();
  }
}
```

#### 5. **Early Exit em Caso de Falha**

```typescript
const essentialScrapers = [olx, zap];
const optionalScrapers = [imovelweb, quintoandar];

// Essencial primeiro
const essentialResults = await Promise.allSettled(
  essentialScrapers.map(s => s.run())
);

// Se algum essencial falhar, aborta
if (essentialResults.some(r => r.status === 'rejected')) {
  logger.error('Essential scraper failed, aborting pipeline');
  process.exit(1);
}

// Roda opcionais com tempo restante
const remainingTime = 6h - elapsed;
await runWithTimeout(optionalScrapers, remainingTime);
```

### ğŸ“Š Benchmarks Esperados

**CenÃ¡rio: 3 scrapers em pipeline de 6h**

| Abordagem | Tempo Total | UtilizaÃ§Ã£o |
|-----------|-------------|------------|
| **Sequencial** | 5h 30min | 92% |
| **Paralelo (3)** | 2h 15min | 38% |
| **Paralelo + Pool** | 1h 50min | 31% |
| **Paralelo + Pool + Checkpoint** | 1h 45min* | 29% |

*Com retry inteligente em caso de falha

### ğŸ¯ ConfiguraÃ§Ã£o Ideal para Pipeline GitHub Actions

```yaml
# .github/workflows/scrape.yml
name: Scrape Data

on:
  schedule:
    - cron: '0 */6 * * *'  # A cada 6h
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6h
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Install Playwright
        run: npx playwright install --with-deps chromium
      
      - name: Load checkpoint
        run: |
          if [ -d "checkpoints" ]; then
            echo "Resuming from checkpoint"
          fi
      
      - name: Run scrapers (parallel)
        run: npm run scrape:parallel
        timeout-minutes: 350  # Deixa 10min de buffer
      
      - name: Save checkpoint
        if: failure()
        run: npm run checkpoint:save
      
      - name: Upload results
        uses: actions/upload-artifact@v3
        with:
          name: scrape-results
          path: output/
```

### ğŸ† Arquitetura Recomendada para Pipeline de 6h

**âœ… Use: Modular Simples + HTTP-first (Cheerio) + Browser Fallback + Checkpointing**

```
src/
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ BaseScraper.ts
â”‚   â”‚   â”œâ”€â”€ BaseHttpScraper.ts      # fetch + cheerio
â”‚   â”‚   â”œâ”€â”€ BrowserPool.ts          # Reuso de browser
â”‚   â”‚   â””â”€â”€ types.ts
â”‚   â”œâ”€â”€ olx/OlxScraper.ts
â”‚   â”œâ”€â”€ zap/ZapScraper.ts
â”‚   â””â”€â”€ registry.ts                  # Lista todos
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ ParallelExecutor.ts          # Orquestrador
â”‚   â”œâ”€â”€ TimeoutManager.ts            # Controle de 6h
â”‚   â”œâ”€â”€ CheckpointManager.ts         # Salvar/carregar
â”‚   â””â”€â”€ PriorityQueue.ts             # ExecuÃ§Ã£o inteligente
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.ts
â”‚   â””â”€â”€ metrics.ts                   # Tracking de tempo
â””â”€â”€ main.ts

checkpoints/                         # Estado persistido
output/                              # Resultados
```

**BenefÃ­cios:**
- âœ… MÃ¡xima performance com paralelizaÃ§Ã£o
- âœ… Usa 100% do tempo disponÃ­vel
- âœ… Retry inteligente com checkpoint
- âœ… Simples de debugar
- âœ… Funciona perfeitamente em GitHub Actions

**Evite:**
- âŒ Queue-Based (overhead desnecessÃ¡rio)
- âŒ DDD complexo (nÃ£o traz ganhos de performance)
- âŒ Scraping sequencial (desperdiÃ§a tempo)

---

## ğŸ”€ MÃºltiplas Pipelines em Paralelo (Conflitos de Git)

### âš ï¸ Problema: Race Condition com Commits SimultÃ¢neos

Se vocÃª rodar **mÃºltiplas pipelines em paralelo** salvando no mesmo repo:

```bash
# Pipeline 1: OLX
git pull
scraperOlx() â†’ data/imoveis.json
git add data/imoveis.json
git commit -m "update olx"
git push  # âœ… Sucesso

# Pipeline 2: ZAP (rodando ao mesmo tempo)
git pull  # NÃ£o vÃª commit do OLX ainda
scraperZap() â†’ data/imoveis.json (SOBRESCREVE!)
git add data/imoveis.json
git commit -m "update zap"
git push  # âŒ CONFLITO! ou pior, sobrescreve OLX
```

**Resultado:** Dados perdidos ou conflitos de merge constantes.

---

## ğŸ¯ SoluÃ§Ãµes para MÃºltiplas Pipelines Paralelas

### âœ… SoluÃ§Ã£o 1: Arquivo Separado por Scraper (Recomendado)

Cada pipeline salva em seu prÃ³prio arquivo e um job final faz merge.

```yaml
# .github/workflows/scrape-olx.yml
name: Scrape OLX
on:
  schedule:
    - cron: '0 */6 * * *'

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Run OLX scraper
        run: npm run scrape:olx
      
      - name: Save to dedicated file
        run: |
          mkdir -p data/scrapers
          mv output/imoveis.json data/scrapers/olx.json
      
      - name: Commit OLX data only
        run: |
          git add data/scrapers/olx.json
          git commit -m "update: olx data"
          git push
```

```yaml
# .github/workflows/scrape-zap.yml (similar, mas zap.json)
# .github/workflows/scrape-vivareal.yml (similar, mas vivareal.json)
```

**Job de Merge (roda apÃ³s todos):**

```yaml
# .github/workflows/merge-data.yml
name: Merge Scraper Data

on:
  workflow_run:
    workflows: ["Scrape OLX", "Scrape ZAP", "Scrape VivaReal"]
    types: [completed]

jobs:
  merge:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Merge all JSON files
        run: |
          node scripts/merge-data.js \
            data/scrapers/olx.json \
            data/scrapers/zap.json \
            data/scrapers/vivareal.json \
            > data/imoveis-merged.json
      
      - name: Commit merged data
        run: |
          git add data/imoveis-merged.json
          git commit -m "merge: all scrapers data"
          git push
```

**Estrutura de Dados:**

```
data/
â”œâ”€â”€ scrapers/           # Dados por scraper
â”‚   â”œâ”€â”€ olx.json
â”‚   â”œâ”€â”€ zap.json
â”‚   â””â”€â”€ vivareal.json
â”œâ”€â”€ imoveis-merged.json # Dados consolidados
â””â”€â”€ metadata.json       # Timestamp, status, etc
```

**Pros:**
- âœ… Zero conflitos
- âœ… FÃ¡cil debugar qual scraper falhou
- âœ… Pode re-rodar scraper individual
- âœ… Merge job valida e deduplica

**Contras:**
- âŒ Mais arquivos no repo
- âŒ Job extra de merge

---

### âœ… SoluÃ§Ã£o 2: Branch por Pipeline + Auto-merge

Cada pipeline trabalha em sua prÃ³pria branch e merge automÃ¡tico no final.

```yaml
# .github/workflows/scrape-olx.yml
name: Scrape OLX

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Create/checkout scraper branch
        run: |
          git checkout -B scraper/olx
          git pull origin main --rebase
      
      - name: Run scraper
        run: npm run scrape:olx
      
      - name: Commit to scraper branch
        run: |
          git add data/
          git commit -m "update: olx data"
          git push origin scraper/olx --force
      
      - name: Create PR or auto-merge
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh pr create \
            --title "Update OLX data" \
            --body "Automated scraper run" \
            --base main \
            --head scraper/olx \
            || gh pr merge scraper/olx --auto --squash
```

**Pros:**
- âœ… Isolamento total entre pipelines
- âœ… Git resolve conflitos automaticamente
- âœ… HistÃ³rico limpo com squash merge

**Contras:**
- âŒ Complexidade de gerenciar branches
- âŒ PR overhead (pode ser resolvido com auto-merge)
- âŒ Conflitos de merge ainda podem acontecer

---

### âœ… SoluÃ§Ã£o 3: Lock DistribuÃ­do (Redis/GitHub API)

Usa lock para garantir que apenas uma pipeline comita por vez.

```typescript
// utils/lockManager.ts
import { Octokit } from '@octokit/rest';

class GitHubLockManager {
  private octokit: Octokit;
  
  async acquireLock(resource: string, timeout = 30 * 60 * 1000): Promise<boolean> {
    // Tenta criar uma issue com label especÃ­fico
    try {
      await this.octokit.issues.create({
        owner: 'ApenasGabs',
        repo: 'querodados',
        title: `LOCK: ${resource}`,
        labels: ['lock', resource],
      });
      return true;
    } catch {
      // Lock jÃ¡ existe, aguarda
      await this.waitForLock(resource, timeout);
      return this.acquireLock(resource, timeout);
    }
  }
  
  async releaseLock(resource: string): Promise<void> {
    const issues = await this.octokit.issues.listForRepo({
      owner: 'ApenasGabs',
      repo: 'querodados',
      labels: ['lock', resource],
      state: 'open',
    });
    
    for (const issue of issues.data) {
      await this.octokit.issues.update({
        owner: 'ApenasGabs',
        repo: 'querodados',
        issue_number: issue.number,
        state: 'closed',
      });
    }
  }
}

// Uso na pipeline
const lock = new GitHubLockManager();

try {
  await lock.acquireLock('data-commit');
  
  // Commit seguro
  await git.pull();
  await git.add('data/imoveis.json');
  await git.commit('update data');
  await git.push();
  
} finally {
  await lock.releaseLock('data-commit');
}
```

**Pros:**
- âœ… Zero conflitos garantido
- âœ… Todas pipelines salvam no mesmo arquivo
- âœ… Simples de entender

**Contras:**
- âŒ Pipelines ficam esperando umas pelas outras
- âŒ Serializa o que era paralelo
- âŒ Deadlock se pipeline quebrar com lock ativo

---

### âœ… SoluÃ§Ã£o 4: Storage Externo + Sync Job (EscalÃ¡vel)

Pipelines salvam em storage externo, job separado faz sync para Git.

```yaml
# .github/workflows/scrape-olx.yml
name: Scrape OLX

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Run scraper
        run: npm run scrape:olx
      
      - name: Upload to S3/Supabase/Firebase
        run: |
          # Salva com timestamp
          curl -X POST $STORAGE_API/scrapers/olx \
            -d @output/imoveis.json \
            -H "Authorization: Bearer $TOKEN"
```

```yaml
# .github/workflows/sync-to-git.yml
name: Sync Storage to Git

on:
  schedule:
    - cron: '0 */1 * * *'  # A cada 1h

jobs:
  sync:
    runs-on: ubuntu-latest
    steps:
      - name: Download latest from all scrapers
        run: |
          curl $STORAGE_API/scrapers/latest > data/imoveis.json
      
      - name: Commit consolidated data
        run: |
          git add data/
          git commit -m "sync: latest data from storage"
          git push
```

**Pros:**
- âœ… Zero conflitos
- âœ… EscalÃ¡vel infinitamente
- âœ… Scrapers totalmente independentes
- âœ… Storage API pode fazer deduplicaÃ§Ã£o

**Contras:**
- âŒ Requer infraestrutura externa
- âŒ Custo adicional (S3, Firebase, etc)
- âŒ Mais complexo

---

## ğŸ† RecomendaÃ§Ã£o Final: MÃºltiplas Pipelines

### Para QueroDADOS (3-5 scrapers)

**Use: SoluÃ§Ã£o 1 - Arquivo Separado por Scraper**

```
data/
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ olx.json        â† Pipeline 1
â”‚   â”œâ”€â”€ zap.json        â† Pipeline 2
â”‚   â””â”€â”€ vivareal.json   â† Pipeline 3
â””â”€â”€ imoveis.json        â† Job de merge

.github/workflows/
â”œâ”€â”€ scrape-olx.yml      â† Roda independente
â”œâ”€â”€ scrape-zap.yml      â† Roda independente
â”œâ”€â”€ scrape-vivareal.yml â† Roda independente
â””â”€â”€ merge-data.yml      â† Consolida tudo
```

**BenefÃ­cios:**
- âœ… Zero conflitos
- âœ… Simples de implementar
- âœ… Sem infraestrutura externa
- âœ… FÃ¡cil debugar
- âœ… Cada pipeline leva 1-2h, todas rodam em paralelo
- âœ… Job de merge leva 2-5min

**Exemplo de Timing:**
```
12:00 - Trigger todas pipelines
12:00-14:00 - OLX scraping (2h)
12:00-13:30 - ZAP scraping (1.5h)
12:00-14:15 - VivaReal scraping (2.15h)
14:15 - Trigger merge job
14:20 - Dados consolidados prontos

Total: 2h 20min vs 6h sequencial
```

### Script de Merge

```typescript
// scripts/merge-data.ts
import fs from 'fs';

interface Imovel {
  id: string;
  source: string;
  title: string;
  price: number;
  // ... outros campos
}

async function mergeScraperData(files: string[]): Promise<Imovel[]> {
  const allData: Imovel[] = [];
  const seenIds = new Set<string>();
  
  for (const file of files) {
    const data = JSON.parse(fs.readFileSync(file, 'utf-8'));
    
    for (const item of data) {
      // Deduplica por ID
      if (!seenIds.has(item.id)) {
        allData.push(item);
        seenIds.add(item.id);
      }
    }
  }
  
  // Ordena por preÃ§o
  allData.sort((a, b) => a.price - b.price);
  
  return allData;
}

// Uso
const merged = await mergeScraperData([
  'data/scrapers/olx.json',
  'data/scrapers/zap.json',
  'data/scrapers/vivareal.json',
]);

fs.writeFileSync('data/imoveis.json', JSON.stringify(merged, null, 2));

console.log(`âœ… Merged ${merged.length} properties from ${files.length} sources`);
```

### Metadata para Tracking

```typescript
// data/metadata.json
{
  "lastUpdate": "2026-01-25T12:20:00Z",
  "sources": {
    "olx": {
      "lastRun": "2026-01-25T12:15:00Z",
      "itemsCount": 150,
      "status": "success"
    },
    "zap": {
      "lastRun": "2026-01-25T12:10:00Z",
      "itemsCount": 200,
      "status": "success"
    },
    "vivareal": {
      "lastRun": "2026-01-25T12:18:00Z",
      "itemsCount": 180,
      "status": "success"
    }
  },
  "totalItems": 530,
  "deduplicated": 0
}
```

---

## ğŸ¯ RecomendaÃ§Ã£o para Novo Projeto

### âš¡ Se Curva de Aprendizado NÃƒO Ã© Problema

**Use: Arquitetura Modular + ParalelizaÃ§Ã£o Interna (Uma Ãšnica Pipeline)**

Rodando **mÃºltiplas pipelines paralelas** pode parecer melhor, mas traz overhead de gerenciamento de conflitos. A melhor abordagem Ã©:

#### âœ… **Uma Pipeline com ParalelizaÃ§Ã£o Interna** (RECOMENDADO)

```yaml
# .github/workflows/scrape-all.yml
name: Scrape All Sources

on:
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6h
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup
        run: |
          npm ci
          npx playwright install chromium
      
      - name: Run all scrapers in parallel
        run: npm run scrape:parallel
        # Internamente roda Promise.allSettled([olx, zap, vivareal])
      
      - name: Merge and validate
        run: npm run merge:validate
      
      - name: Commit results
        run: |
          git config user.name "Bot"
          git config user.email "bot@example.com"
          git add data/
          git commit -m "update: scrape data $(date +%Y-%m-%d)"
          git push
```

**Vantagens vs MÃºltiplas Pipelines:**
- âœ… **Zero conflitos de Git** (um Ãºnico commit)
- âœ… **Mesma performance** (paralelizaÃ§Ã£o interna com Promise.allSettled)
- âœ… **Menos overhead** (uma VM ao invÃ©s de 3-5)
- âœ… **Mais simples de debugar** (logs em um Ãºnico lugar)
- âœ… **Controle de timeout unificado** (6h para tudo)
- âœ… **Commit atÃ´mico** (ou tudo funciona ou nada)

**Estrutura Interna:**

```typescript
// src/main.ts
import { BrowserPool } from './pipeline/BrowserPool';
import { ParallelExecutor } from './pipeline/ParallelExecutor';
import { scraperRegistry } from './scrapers/registry';

async function main() {
  const pool = new BrowserPool({ maxBrowsers: 3 });
  await pool.initialize();
  
  try {
    // Executa todos scrapers em paralelo
    const executor = new ParallelExecutor(pool);
    const results = await executor.runAll(scraperRegistry);
    
    // Merge e validaÃ§Ã£o
    const merged = mergeScraper Data(results);
    await validateData(merged);
    
    // Salva resultado final
    await saveToFile('data/imoveis.json', merged);
    await saveMetadata(results);
    
  } finally {
    await pool.cleanup();
  }
}
```

#### ğŸ“Š Performance Comparativa Real

**CenÃ¡rio: 3 scrapers (OLX 2h, ZAP 1.5h, VivaReal 2.15h)**

| Abordagem | Tempo Total | VMs/Runners | Conflitos Git | Complexidade |
|-----------|-------------|-------------|---------------|--------------|
| **Sequencial** | 5h 45min | 1 | 0 | â­â­â­â­â­ |
| **Paralelo Interno** | 2h 15min | 1 | 0 | â­â­â­â­ |
| **MÃºltiplas Pipelines** | 2h 15min | 3 | Alto risco | â­â­ |

âœ… **Paralelo Interno ganha**: mesma performance, menos complexidade, zero conflitos.

---

### ğŸ—ï¸ Arquitetura Recomendada (Sem Medo de Complexidade)

**Use: Modular + DDD Lite + ParalelizaÃ§Ã£o**

```
src/
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ BaseScraper.ts           # Abstract class
â”‚   â”‚   â”œâ”€â”€ ScraperConfig.ts
â”‚   â”‚   â””â”€â”€ types.ts
â”‚   â”œâ”€â”€ olx/
â”‚   â”‚   â”œâ”€â”€ OlxScraper.ts
â”‚   â”‚   â”œâ”€â”€ olx.config.ts
â”‚   â”‚   â”œâ”€â”€ selectors.ts             # Seletores isolados
â”‚   â”‚   â”œâ”€â”€ mapper.ts                # Raw â†’ Domain
â”‚   â”‚   â””â”€â”€ validator.ts             # ValidaÃ§Ã£o especÃ­fica
â”‚   â”œâ”€â”€ zap/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ vivareal/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ registry.ts                  # Registra todos
â”‚
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ Imovel.ts                    # Entity principal
â”‚   â”œâ”€â”€ Price.ts                     # Value Object
â”‚   â”œâ”€â”€ Location.ts                  # Value Object
â”‚   â””â”€â”€ events/
â”‚       â””â”€â”€ ImovelScraped.ts         # Domain event
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ BrowserPool.ts               # Gerencia Playwright instances
â”‚   â”œâ”€â”€ ParallelExecutor.ts          # Orquestra scrapers
â”‚   â”œâ”€â”€ TimeoutManager.ts            # Controla 6h limit
â”‚   â”œâ”€â”€ CheckpointManager.ts         # Salva progresso
â”‚   â”œâ”€â”€ MergeService.ts              # Merge + deduplica
â”‚   â””â”€â”€ ValidationService.ts         # Valida dados finais
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ FileStorage.ts               # Salva JSON
â”‚   â”œâ”€â”€ Logger.ts                    # Winston estruturado
â”‚   â”œâ”€â”€ MetricsCollector.ts          # Tracking
â”‚   â””â”€â”€ GitCommitter.ts              # Commit automÃ¡tico
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ retry.ts                     # Retry com backoff
â”‚   â”œâ”€â”€ timeout.ts                   # Promise timeout
â”‚   â””â”€â”€ delay.ts                     # Rate limiting
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ index.ts                     # Config geral
â”‚   â””â”€â”€ scrapers.ts                  # Config por scraper
â”‚
â””â”€â”€ main.ts                          # Entry point limpo
```

**Por que essa arquitetura?**

1. **Domain Layer** â†’ Garante consistÃªncia de dados
2. **Pipeline Layer** â†’ Performance com paralelizaÃ§Ã£o controlada
3. **Infrastructure Layer** â†’ Logging, storage, git separados
4. **Scraper especÃ­fico isolado** â†’ FÃ¡cil adicionar/remover

**Complexidade justificada:**
- âœ… SeparaÃ§Ã£o clara de responsabilidades
- âœ… FÃ¡cil testar cada parte
- âœ… Escala para 10+ scrapers sem problemas
- âœ… ManutenÃ­vel no longo prazo

---

### ğŸ’ Exemplo de ImplementaÃ§Ã£o Core

#### BaseScraper.ts

```typescript
import type { Page, BrowserContext } from 'playwright';
import type { ScraperConfig } from './ScraperConfig';
import type { RawData, Imovel } from '../domain/types';

export abstract class BaseScraper<T extends RawData = RawData> {
  constructor(protected config: ScraperConfig) {}
  
  abstract name: string;
  abstract baseUrl: string;
  
  // Template method pattern
  async run(context: BrowserContext): Promise<Imovel[]> {
    const page = await context.newPage();
    
    try {
      await this.setup(page);
      const rawData = await this.scrape(page);
      const mapped = await this.map(rawData);
      const validated = await this.validate(mapped);
      return validated;
      
    } catch (error) {
      await this.handleError(page, error);
      throw error;
      
    } finally {
      await page.close();
    }
  }
  
  protected async setup(page: Page): Promise<void> {
    await page.setViewportSize({ width: 1920, height: 1080 });
    await page.setExtraHTTPHeaders({
      'User-Agent': this.config.userAgent,
    });
  }
  
  protected abstract scrape(page: Page): Promise<T[]>;
  protected abstract map(raw: T[]): Promise<Imovel[]>;
  protected abstract validate(data: Imovel[]): Promise<Imovel[]>;
  
  protected async handleError(page: Page, error: Error): Promise<void> {
    await page.screenshot({ path: `logs/${this.name}-error.png` });
    logger.error(`${this.name} failed`, { error });
  }
}
```

#### ParallelExecutor.ts

```typescript
import type { BrowserPool } from './BrowserPool';
import type { BaseScraper } from '../scrapers/base/BaseScraper';
import { TimeoutManager } from './TimeoutManager';

export class ParallelExecutor {
  constructor(
    private pool: BrowserPool,
    private timeoutManager = new TimeoutManager(6 * 60 * 60 * 1000)
  ) {}
  
  async runAll(scrapers: BaseScraper[]): Promise<ScraperResult[]> {
    const results = await Promise.allSettled(
      scrapers.map(async (scraper) => {
        const context = await this.pool.getContext();
        
        try {
          // Timeout individual por scraper (5h)
          const data = await this.timeoutManager.withTimeout(
            scraper.run(context),
            scraper.config.maxDuration
          );
          
          return {
            scraper: scraper.name,
            status: 'success' as const,
            data,
            duration: Date.now() - startTime,
          };
          
        } catch (error) {
          return {
            scraper: scraper.name,
            status: 'failed' as const,
            error: error.message,
            duration: Date.now() - startTime,
          };
          
        } finally {
          await this.pool.releaseContext(context);
        }
      })
    );
    
    return results.map(r => r.status === 'fulfilled' ? r.value : r.reason);
  }
}
```

#### BrowserPool.ts

```typescript
import { chromium, type Browser, type BrowserContext } from 'playwright';

export class BrowserPool {
  private browser: Browser | null = null;
  private contexts: BrowserContext[] = [];
  private availableContexts: BrowserContext[] = [];
  
  constructor(private config: { maxBrowsers: number }) {}
  
  async initialize(): Promise<void> {
    this.browser = await chromium.launch({
      headless: true,
      args: ['--no-sandbox', '--disable-setuid-sandbox'],
    });
    
    // Pre-cria contextos
    for (let i = 0; i < this.config.maxBrowsers; i++) {
      const context = await this.browser.newContext();
      this.contexts.push(context);
      this.availableContexts.push(context);
    }
  }
  
  async getContext(): Promise<BrowserContext> {
    while (this.availableContexts.length === 0) {
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
    return this.availableContexts.pop()!;
  }
  
  async releaseContext(context: BrowserContext): Promise<void> {
    this.availableContexts.push(context);
  }
  
  async cleanup(): Promise<void> {
    await Promise.all(this.contexts.map(c => c.close()));
    await this.browser?.close();
  }
}
```

---

### ğŸ¯ DecisÃ£o Final

Se **curva de aprendizado nÃ£o Ã© problema** e vocÃª quer **mÃ¡xima performance com mÃ­nimo de fricÃ§Ã£o**:

#### âœ… **RecomendaÃ§Ã£o: Modular + DDD Lite + ParalelizaÃ§Ã£o Interna**

**Motivos:**
1. **Performance mÃ¡xima** â†’ 3x mais rÃ¡pido que sequencial
2. **Zero conflitos Git** â†’ Uma pipeline, um commit
3. **EscalÃ¡vel** â†’ Adiciona scrapers facilmente
4. **TestÃ¡vel** â†’ Cada camada isolada
5. **Profissional** â†’ Estrutura enterprise-grade
6. **ManutenÃ­vel** â†’ CÃ³digo organizado e limpo

**NÃ£o Use:**
- âŒ MÃºltiplas pipelines paralelas (overhead de conflitos)
- âŒ Queue-based (complexidade desnecessÃ¡ria para job de 6h)
- âŒ Sequencial (desperdiÃ§a 3-4h de tempo)

**PrÃ³ximo Passo:**
Posso criar o template completo com essa arquitetura se quiser comeÃ§ar a implementar!

---

## ğŸ› ï¸ Stack Recomendado para Cada Arquitetura

### Modular Simples
```json
{
  "dependencies": {
    "playwright": "^1.40.0",
    "winston": "^3.11.0",
    "joi": "^17.0.0",
    "dotenv": "^16.0.0"
  },
  "devDependencies": {
    "typescript": "^5.3.0",
    "vitest": "^1.0.0",
    "ts-node": "^10.9.0"
  }
}
```

### DDD
```json
{
  "dependencies": {
    "playwright": "^1.40.0",
    "winston": "^3.11.0",
    "joi": "^17.0.0",
    "tsyringe": "^4.8.0",
    "class-transformer": "^0.5.1",
    "class-validator": "^0.14.0",
    "dotenv": "^16.0.0"
  },
  "devDependencies": {
    "typescript": "^5.3.0",
    "vitest": "^1.0.0",
    "ts-node": "^10.9.0"
  }
}
```

### Plugin-Based
```json
{
  "dependencies": {
    "playwright": "^1.40.0",
    "winston": "^3.11.0",
    "joi": "^17.0.0",
    "tsyringe": "^4.8.0",
    "eventemitter3": "^5.0.0",
    "dotenv": "^16.0.0"
  },
  "devDependencies": {
    "typescript": "^5.3.0",
    "vitest": "^1.0.0",
    "ts-node": "^10.9.0"
  }
}
```

### Queue-Based
```json
{
  "dependencies": {
    "playwright": "^1.40.0",
    "winston": "^3.11.0",
    "joi": "^17.0.0",
    "bull": "^4.11.0",
    "ioredis": "^5.3.0",
    "tsyringe": "^4.8.0",
    "dotenv": "^16.0.0"
  },
  "devDependencies": {
    "typescript": "^5.3.0",
    "vitest": "^1.0.0",
    "ts-node": "^10.9.0"
  }
}
```

---

## ğŸ’¡ PrincÃ­pios Fundamentais

Independente da arquitetura escolhida:

### 1. **SeparaÃ§Ã£o de Responsabilidades**
```typescript
// âŒ Ruim - Tudo misturado
async function scrape() {
  const page = await browser.newPage();
  await page.goto(url);
  const data = await page.evaluate(...);
  // Scraping + Mapping + ValidaÃ§Ã£o + Storage
}

// âœ… Bom - Cada coisa no seu lugar
async function scrape() {
  const raw = await scraper.fetch(url);
  const mapped = mapper.toImovel(raw);
  validator.validate(mapped);
  await storage.save(mapped);
}
```

### 2. **ConfiguraÃ§Ã£o Centralizada**
```typescript
// config/index.ts
export const scraperConfig = {
  olx: { timeout: 30000, retries: 3 },
  zap: { timeout: 25000, retries: 2 },
};
```

### 3. **Logging Estruturado**
```typescript
logger.info('Scraping started', { scraper: 'olx', url });
logger.error('Failed to fetch', { error, retries: 2 });
```

### 4. **ValidaÃ§Ã£o de Dados**
```typescript
// Sempre validar dados coletados
const schema = joi.object({
  title: joi.string().required(),
  price: joi.number().positive().required(),
});
```

### 5. **Error Handling Robusto**
```typescript
try {
  await scraper.run();
} catch (error) {
  if (isRecoverable(error)) {
    await retry();
  } else {
    await notifyOps();
  }
}
```

### 6. **Sem DuplicaÃ§Ã£o**
- LÃ³gica compartilhada em `BaseScraper`
- UtilitÃ¡rios em `utils/`
- Config centralizada

---

## ğŸš€ PrÃ³ximos Passos

Recomendo comeÃ§ar com a **Arquitetura Modular Simples** e evoluir conforme necessÃ¡rio.

Se quiser, posso criar um template starter com essa arquitetura!


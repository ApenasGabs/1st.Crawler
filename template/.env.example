# Variáveis de Ambiente - 1st.Crawler Template

# ========== BROWSER ==========
# Número de contextos de browser para paralelização (recomendado: 2-5 em CI, 1-3 em local)
BROWSER_CONTEXTS=3

# Timeout por scraper em segundos (defina conforme o site mais lento)
SCRAPER_TIMEOUT=300

# Usar headless mode (true em CI, false para debug local)
BROWSER_HEADLESS=true

# ========== DADOS ==========
# Diretório onde os arquivos JSON de saída serão salvos
DATA_OUTPUT_DIR=data/scrapers

# Validação obrigatória (false = continua mesmo com dados inválidos)
VALIDATE_DATA=true

# ========== LOGGING ==========
# Nível de log: debug, info, warn, error
LOG_LEVEL=info

# Salvar logs em arquivo
LOG_FILE=logs/scraper.log

# ========== TIMEOUT & RETRY ==========
# Máximo de tentativas por scraper
MAX_RETRIES=2

# Delay entre tentativas em segundos
RETRY_DELAY=5

# ========== CI/CD ==========
# Token do GitHub para fazer push (secrets do GitHub Actions)
# GITHUB_TOKEN=seu_token_aqui

# Commit automático de dados após scraping (true em CI)
AUTO_COMMIT=false

# ========== SCRAPING ESPECÍFICO ==========
# User-Agent customizado (deixe vazio para padrão do Playwright)
USER_AGENT=

# Timeout de navegação em milissegundos
NAVIGATION_TIMEOUT=30000

# Proxy URL (opcional)
# PROXY_URL=http://proxy:3128

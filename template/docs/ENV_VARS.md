# Documenta√ß√£o de Vari√°veis de Ambiente

## Setup Inicial

1. Copie `.env.example` para `.env`:
   ```bash
   cp .env.example .env
   ```

2. Customize conforme sua necessidade

## Vari√°veis por Contexto

### üñ•Ô∏è Local Development

```bash
BROWSER_CONTEXTS=1
BROWSER_HEADLESS=false
LOG_LEVEL=debug
SCRAPER_TIMEOUT=600
AUTO_COMMIT=false
```

Para debug, voc√™ pode ver o browser aberto enquanto scrape.

### üöÄ CI/CD (GitHub Actions)

```bash
BROWSER_CONTEXTS=3
BROWSER_HEADLESS=true
LOG_LEVEL=info
SCRAPER_TIMEOUT=300
AUTO_COMMIT=true
VALIDATE_DATA=true
```

‚ö†Ô∏è **Importante**: Configure esses secrets no GitHub:
- `GIT_USER_NAME` e `GIT_USER_EMAIL`: para commit autom√°tico
- `GITHUB_TOKEN`: para fazer push (ou use o token default)

### üîß Performance Tuning

| Vari√°vel | Impacto | Trade-off |
|---|---|---|
| `BROWSER_CONTEXTS` | Mais contextos = mais paralelo | ‚ö†Ô∏è Mais RAM, poss√≠veis timeouts |
| `SCRAPER_TIMEOUT` | Timeout por scraper | ‚ö†Ô∏è Muito curto = falhas, muito longo = espera desnecess√°ria |
| `MAX_RETRIES` | Resili√™ncia | ‚ö†Ô∏è Mais tentativas = mais tempo de pipeline |
| `RETRY_DELAY` | Espera entre retentativas | ‚ö†Ô∏è Delay muito curto pode ser bloqueado |

### üìä Recomenda√ß√µes por Escala

#### Pequeno (1-2 scrapers, <1h de dados)
```bash
BROWSER_CONTEXTS=2
SCRAPER_TIMEOUT=300
MAX_RETRIES=1
```

#### M√©dio (3-5 scrapers, ~2h de dados)
```bash
BROWSER_CONTEXTS=3
SCRAPER_TIMEOUT=600
MAX_RETRIES=2
```

#### Grande (6+ scrapers, ~4h de dados)
```bash
BROWSER_CONTEXTS=5
SCRAPER_TIMEOUT=900
MAX_RETRIES=2
VALIDATE_DATA=false  # Para ganhar tempo se j√° validou localmente
```

### üîê Secrets do GitHub Actions

No reposit√≥rio, v√° para **Settings ‚Üí Secrets and variables ‚Üí Actions** e configure:

```yaml
GIT_USER_NAME: "crawler-bot"
GIT_USER_EMAIL: "crawler-bot@users.noreply.github.com"
GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # Token autom√°tico do Actions
```

## Exemplo .env Completo

```bash
# Browser
BROWSER_CONTEXTS=3
SCRAPER_TIMEOUT=300
BROWSER_HEADLESS=true

# Dados
DATA_OUTPUT_DIR=data/scrapers
VALIDATE_DATA=true

# Logging
LOG_LEVEL=info
LOG_FILE=logs/scraper.log

# Retry
MAX_RETRIES=2
RETRY_DELAY=5

# CI/CD
AUTO_COMMIT=false

# Scraping
USER_AGENT=
NAVIGATION_TIMEOUT=30000
```

## Troubleshooting

### "Timeout esperando p√°gina carregar"
‚Üí Aumentar `SCRAPER_TIMEOUT` ou `NAVIGATION_TIMEOUT`

### "Muitos contextos causando crash"
‚Üí Reduzir `BROWSER_CONTEXTS`

### "Dados incompletos/inv√°lidos"
‚Üí Ativar `LOG_LEVEL=debug` e revisar os logs

### "CI/CD n√£o faz push"
‚Üí Verificar `GITHUB_TOKEN` e permiss√µes do bot
